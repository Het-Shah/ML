{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "print(fmnist_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "print(fmnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total trainning batch number: 600\n",
      "==>>> total testing batch number: 100\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=fmnist_trainset,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=fmnist_testset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "\n",
    "print('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,20,5,1)\n",
    "        self.conv2 = nn.Conv2d(20,50,5,1)\n",
    "        self.fc1 = nn.Linear(4*4*50,500)\n",
    "        self.fc2 = nn.Linear(500,10)\n",
    "    def forward(self, y):\n",
    "        y = F.relu(self.conv1(y))\n",
    "        y = F.max_pool2d(y, 2, 2)\n",
    "        y = F.relu(self.conv2(y))\n",
    "        y = F.max_pool2d(y, 2, 2)\n",
    "        y = y.view(-1, 4*4*50)\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 1, batch index: 600, train loss: 0.293539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/het/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 1, batch index: 100, test loss: 1.039394, acc: 7603\n",
      "==>>> epoch: 2, batch index: 600, train loss: 0.152221\n",
      "==>>> epoch: 2, batch index: 100, test loss: 0.961841, acc: 7784\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(),lr=0.001)\n",
    "for epoch in range(10):\n",
    "    # training\n",
    "    ave_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimiser.zero_grad()\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "#         print(out.shape)\n",
    "#         print(target.shape)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        if (batch_idx+1) % 600 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(epoch+1, batch_idx+1, loss))\n",
    "    # testing\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        x, target = Variable(x, volatile=True), Variable(target, volatile=True)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "#         total_cnt += x.data.size()[0]\n",
    "#         total_cnt += 100\n",
    "#         print('Prediction: {}'.format(pred_label))\n",
    "#         print(\"Target: {}\".format(target.data))\n",
    "        correct_cnt += (pred_label == target.data).sum()\n",
    "#         print(\"Correct_cnt: {}\".format(correct_cnt))\n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {}'.format(epoch+1, batch_idx+1, loss, correct_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_cnt: 7910\n"
     ]
    }
   ],
   "source": [
    "print(\"correct_cnt: {}\".format(correct_cnt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
